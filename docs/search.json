[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preregistration and Exploratory Analysis",
    "section": "",
    "text": "Preface\nPlease note that this tutorial is still under active development. Please check back regularly for changes and updates.\n\nPreregistration is an important research standard to guard against questionable research practices and protect the integrity of statistical inferences. But there are parts of a new study that can be difficult or impossible to define in advance, particularly when both exploratory data analysis (EDA) and confirmatory data analysis (CDA) are needed to advance the research program.\nIn this tutorial, we will discuss how to use preregistration to help you plan both exploratory and confirmatory work, addressing two common sources of frustration for researchers developing new projects: model specification and sample size estimation.\nIn particular, we will showcase the value of exploratory work for hypothesis generation as part of the iterative scientific process. Exploratory studies and analyses help researchers refine and specify their research questions and statistical hypotheses, enabling them to properly design, preregister, and conduct confirmatory studies and ultimately build stronger theories of scientific understanding.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Preregistration has become an important standard in social science research to protect the researcher and the broader scientific community from engaging in research practices that introduce systemic bias into the research process (John, Loewenstein, and Prelec 2012; Lakens et al., n.d.; Nosek et al. 2018). Such practices can include the p-hacking of statistical tests (Simmons, Nelson, and Simonsohn 2011), hypothesizing after the results are known (referred to as HARKing, see Kerr 1998), and the cherry-picking of data, variables, or statistically significant results. By preregistering your methods, sample, and analysis plan before conducting a new study, you maintain a level of transparency in your research workflow that helps reduce the researcher degrees of freedom (Simmons, Nelson, and Simonsohn 2011)—excessive flexibility in your research workflow—that can intentionally or inadvertently enable these problematic practices to persist.\nNevertheless, there are parts of a new study that can be challenging to define in advance. If your new study is taking first steps into uncharted scientific waters, you may not be able to know which statistical analysis or model class will be best suited to your data until you have the data in hand. In these situations, preregistration can feel like an obstacle: you can’t proceed to data collection without stating your intended analysis plan, but you can’t formalize your analysis plan without data-dependent information. So, is preregistration still a helpful practice here?\nYes! In fact, one of the intended functions of the preregistration process is to make researchers distinguish between two aspects of a research program that should in principle be separated: exploratory and confirmatory work.\nHere is how these two concepts are described by the Open Science Framework (OSF) in the context of preregistration:\n\n“Preregistration separates hypothesis-generating (exploratory) from hypothesis-testing (confirmatory) research. Both are important. But the same data cannot be used to generate and test a hypothesis, which can happen unintentionally and reduce the credibility of your results.”\n\nIf you are not already incorporating a designated exploratory study into your research plan, the prospect of doing so can feel daunting—where you might have expected (and perhaps budgeted) to collect and analyze just one dataset, you now need two separate studies before you can answer your research question. But this attitude undermines the importance of exploratory research! Before you can worry about the answer to your research question, you need exploratory research to help you be sure that you are asking the right question at all.\nFurthermore, while you may require separate samples to conduct your exploratory and confirmatory analyses, the total analysis work need not be twice as burdensome. As long as you are clear about which aspects of your research program are exploratory and which are confirmatory, you can plan (and preregister!) your work efficiently and effectively.\nIn this tutorial, we will discuss strategies for balancing exploratory and confirmatory statistical analyses during study planning and preregistration. We will review the distinctions between exploratory and confirmatory analyses and the different analytical goals they involve, such as description, hypothesis generation, inference, and out-of-sample prediction. From there, we will discuss two aspects of new study design that can be the most challenging to preregister: statistical model specification and sample size estimation. We will use the toolkit of exploratory analysis to help make addressing the challenges of research planning easier as you set up and preregister your research program.\n\n\n\n\nJohn, Leslie K., George Loewenstein, and Drazen Prelec. 2012. “Measuring the Prevalence of Questionable Research Practices With Incentives for Truth Telling.” Psychological Science 23 (5): 524–32. https://doi.org/10.1177/0956797611430953.\n\n\nKerr, Norbert L. 1998. “HARKing: Hypothesizing After the Results Are Known.” Personality and Social Psychology Review 2 (3): 196–217. https://doi.org/10.1207/s15327957pspr0203_4.\n\n\nLakens, Daniel, Cristian Mesquida, Sajedeh Rasti, and Massimiliano Ditroilo. n.d. “The Benefits of Preregistration and Registered Reports.” https://doi.org/10.31234/osf.io/dqap7.\n\n\nNosek, Brian A., Charles R. Ebersole, Alexander C. DeHaven, and David T. Mellor. 2018. “The Preregistration Revolution.” Proceedings of the National Academy of Sciences of the United States of America 115 (11): 2600–2606. https://doi.org/10.1073/pnas.1708274114.\n\n\nSimmons, Joseph P., Leif D. Nelson, and Uri Simonsohn. 2011. “False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant.” Psychological Science 22 (11): 1359–66. https://doi.org/10.1177/0956797611417632.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "research_processes.html",
    "href": "research_processes.html",
    "title": "Research Processes",
    "section": "",
    "text": "Explore, Hypothesize, Infer, (Dis)Confirm, Repeat.\nExploratory and confirmatory analyses constitute two overarching objectives of research inquiry. However, there are several steps in the iterative research process that connect the exploratory and confirmatory parts of the scientific process.\nThe key steps in this process are hypothesis generation and hypothesis testing.\nIn the following section, we will review the exploratory steps and the confirmatory steps using some examples of each type of question and its corresponding analyses.\nThe goal is not to partition all research into either exploratory (hypothesis-generating) research or confirmatory (hypothesis-testing) research—such a task is likely not even possible (Fife and Rodgers 2022; Jacobucci 2022). Instead, we will review strategies for identifying and differentiating the exploratory and confirmatory objectives within a research program, bearing in mind that most research programs will contain both exploratory and confirmatory lines of inquiry at various stages.\n\n\n\n\nFife, Dustin A., and Joseph Lee Rodgers. 2022. “Understanding the Exploratory/Confirmatory Data Analysis Continuum: Moving Beyond the “Replication Crisis”.” American Psychologist 77 (3): 453–66. https://doi.org/10.1037/amp0000886.\n\n\nJacobucci, Ross. 2022. “A Critique of Using the Labels Confirmatory and Exploratory in Modern Psychological Research.” Frontiers in Psychology 13. https://www.frontiersin.org/articles/10.3389/fpsyg.2022.1020770.",
    "crumbs": [
      "Research Processes"
    ]
  },
  {
    "objectID": "hypothesis_generation.html",
    "href": "hypothesis_generation.html",
    "title": "1  Hypothesis Generation",
    "section": "",
    "text": "Observe, describe, and build hypotheses.\nHypothesis generation is the purview of exploratory work. Hypotheses can sometimes be derived directly from highly specified theories, but in the messy world of the behavioral and social sciences, it is more common to construct hypothesis on the basis of patterns observed in exploratory datasets or selected from our experiences in the world.\n\n\n\n\nJohn Wilder Tukey\n\n\n\nThe principle of separating exploratory and confirmatory research into distinct lines of inquiry—crucially, inquiries conducted on separate datasets—derives from the original formalization of exploratory data analysis (EDA) developed by John Tukey in the 1970s-80s (Tukey 1970, 1980a). Tukey championed the value of questions that ask, “what is \\(X\\) like?” or “what characterizes \\(X\\)?” These types of questions orient us to analyses that help us document the existence of a phenomenon, describe its features, quantify it, and summarize it.\n\n\nDescriptive analysis: charting new territory\nIn some cases, description and quantification of a phenomenon might itself satisfy your immediate research question. This is often the case when description alone is a considerable challenge, such as when a dataset is especially large or complex. The toolbox of descriptive analysis includes many techniques for summarizing and organizing a dataset to reveal its structure in human-readable terms.\n\nInitial description: summary statistics and distribution visualizations\nLet’s use an example dataset from the datasets package in R to illustrate some of these descriptive analyses in action. The attitude dataset contains aggregated data from the Chatterjee-Price attitude survey, in which clerical employees across 30 departments of a large organization were asked to evaluate their employers on a series of topics. Each row of the data frame corresponds to one department, and the values in each column are the percent of favorable responses to each question from across employees sampled in a given department. The dataset is previewed in Table 1.1 (I’ve renamed the variables based on the documentation for interpretability).\n\n\n\n\nTable 1.1\n\n\n\n\n\n\n\n\n\n\nAs a first step in organizing these data to understand them better, we typically compute summary statistics: metrics that help characterize the distributions of our variables. These are generally metrics indicating the location (mean, median, etc.) and dispersion (standard deviation, quartile range, etc.) of a given variable’s distribution. Table 1.2 is an example summary table for our employee attitude data (computed using the sumtable() function from the vtable package in R):\n\n\n\n\nTable 1.2\n\n\n\n\n\n\n\n\n\n\nThese summary metrics are already an improvement over the raw data alone in our attempt to characterize the data, but we can do much better by adding visualizations. Even simple descriptive visualizations like the histograms shown in Figure 1.1 can help give us a better impression of each of the distributions in the data.\n\n\n\n\n\n\n\n\nFigure 1.1: Histograms of variable distributions in attitude survey dataset.\n\n\n\n\n\n\n\nComplex description: multivariate exploratory analyses\nNow that we have a better understanding of the individual distributions of each variable, we can use more complex descriptive tools to look at the relationships among the variables, describing the phenomenon more fully. For example, we might compute the relative similarities between each variable and visualize those relationships using tools like cluster analysis, often in conjunction with dimensionality-reduction techniques.\nBelow is an example of these more advanced exploratory methods applied our attitude data. For this example, we have invented three categories for the variables in the data: “General” for the items “Overall rating” and “Too critical”, “Management” for the items “Handling of employee complaints” and “Does not allow special privileges”, and “Growth” for the items “Opportunity to learn”, “Raises based on performance”, and “Advancement”. With this category structure in our data, we can combine two techniques to see whether the multivariate relationships in the data reflect the expected category structure. The first technique is multidimensional scaling (MDS), which allows us to embed the relationships between all the variables in a 2-dimensional scatter plot for easy visualization. The second technique is a type of machine learning classification method called support vector machine (SVM) classification, which will try to place the variables into categories based on the relationships in the data. (see Mair, Cetron, and Borg 2022 for a tutorial paper on the combined use of these two methods.)\n\n\n\n\n\n\n\n\nFigure 1.2: Exploratory classification analysis plot showing multidimensional scaling (MDS) used together with support vector machine (SVM) classification on our modified attitude example data.\n\n\n\n\n\nFigure 1.2 shows the results of the exploratory MDS/SVM analysis. The color-coded regions reflect the SVM classifier’s predicted category assignments, and the point shapes represent the actual category assignments. As we can see, the data align somewhat well with the expected categories, although not perfectly—the classifier places “Overall rating” and “Opportunity to learn” incorrectly into the “Management” category, due to apparent similarities between those variables and the other “Management” variables.\nMultivariate exploratory methods like these can also be used to reveal structures in the data that are not necessarily known in advance, and often in much larger datasets. In genetics research, for example, researchers will characterize cell samples based on the cells’ different transcription profiles across thousands or tens of thousands of different genes, hoping to identify sensible structures through exploratory analyses. To achieve a working description of their complex sample data, biologists will use techniques like principal components analysis (PCA) to organize cell samples based on variance in their gene expression patterns, aiming to group together cells that express similar gene profiles (see Piper et al. 2023 for a basic tutorial).\n\n\n\n\n\n\nFigure 1.3: PCA in genetics research\n\n\n\nFigure 1.3 shows an example of this analysis in action in a genetics paper from Pollen et al. (2014) (this figure is discussed as an example in this tutorial video). In this example, PCA is used to differentiate cell types based on gene transcription heterogeneity.\nMethods like PCA, MDS, SVM classification, and other similar techniques are powerful descriptive methods for the current era of “big data”, which continues to grow and encompass more and more scientific fields. These methods can help make complex, feature-rich data more approachable and provide a launchpad for the generation of hypotheses.\n\n\n\nFrom description to hypothesis\nIn many cases, exploratory description is a prerequisite step to start building up a research program so that you can design subsequent confirmatory analyses. Particularly if your research constitutes the very first inquiry into a new topic area, you might need to dedicate multiple early studies to exploration and description alone before you are ready to generate and test specific hypotheses. Before you can start worrying about statistical power, sample sizes, and p-values, you need to decide what question you want to ask, and how it should be formalized into a statistical model or test.\nThis process—of using exploration to inform hypothesis generation—is complex. And yet, we often mislead ourselves into thinking it is straightforward. To correct this misconception, we can again turn to Tukey, who articulated his concern about this issue in his piece, “We Need Both Exploratory and Confirmatory,” published in The American Statistician in 1980. In that article, Tukey emphasized that the scientific method is not linear, but iterative (Tukey 1980b), and illustrated this distinction using a series of diagrams (reproduced below with adjusted formatting):\n\n“We are, I assert, all too familiar with the following straight-line paradigm—asserted far too frequently as how science and engineering function:\n\\((*) \\; \\text{question} \\rightarrow \\text{design} \\rightarrow \\text{collection} \\rightarrow \\text{analysis} \\rightarrow \\text{answer}\\)\n…\nWhat often happens is better diagrammed thus:\n\\((*) \\; \\text{idea} \\rightarrow \\, \\circlearrowright ({\\text{question} \\atop \\text{design}})\\circlearrowright \\, \\rightarrow \\text{collection} \\rightarrow \\text{analysis} \\rightarrow \\text{answer}\\)”\n\nTukey goes on to explain that there is an important difference between having an idea of a question (which we can describe in words) and having a statistically-supportable question (which we can describe in hypothesis terms). In order to obtain a research question that is ready for confirmatory analysis, we may have to repeat the exploratory process many times, refining our question and design along the way.\n\n\nPost-hoc exploratory analysis\nFinally, exploratory analysis can be a follow-up to a completed confirmatory analysis to motivate new, subsequent lines of inquiry (see Fife and Rodgers 2022). It is important to always be clear about exactly which analyses are exploratory in this context, so as not to imply that inferential conclusions should be drawn from them. Nevertheless, researchers should not shy away from using exploratory analysis as a generative tool for guiding the long-term development of a research program.\n\n\n\n\nFife, Dustin A., and Joseph Lee Rodgers. 2022. “Understanding the Exploratory/Confirmatory Data Analysis Continuum: Moving Beyond the “Replication Crisis”.” American Psychologist 77 (3): 453–66. https://doi.org/10.1037/amp0000886.\n\n\nMair, Patrick, Joshua S. Cetron, and Ingwer Borg. 2022. “Using Support Vector Machines for Facet Partitioning in Multidimensional Scaling.” Multivariate Behavioral Research 0 (0): 1–17. https://doi.org/10.1080/00273171.2022.2035207.\n\n\nPiper, Mary, Radhika Khetani, Meeta Mistry, Jihe Liu, Amélie Julé, Gammerdinger, Darío Hereñú, Ilya Sytchev, and kew24. 2023. Hbctraining/scRNA-Seq_online: Single Cell RNA-Seq Analysis. Zenodo. https://doi.org/10.5281/zenodo.7723318.\n\n\nPollen, Alex A., Tomasz J. Nowakowski, Joe Shuga, Xiaohui Wang, Anne A. Leyrat, Jan H. Lui, Nianzhen Li, et al. 2014. “Low-Coverage Single-Cell mRNA Sequencing Reveals Cellular Heterogeneity and Activated Signaling Pathways in Developing Cerebral Cortex.” Nature Biotechnology 32 (10): 1053–58. https://doi.org/10.1038/nbt.2967.\n\n\nTukey, John W. 1970. Exploratory Data Analysis. Addison Wesley Publishing Company.\n\n\n———. 1980a. “We Need Both Exploratory and Confirmatory.” The American Statistician 34 (1): 23–25. https://doi.org/10.1080/00031305.1980.10482706.\n\n\n———. 1980b. “We Need Both Exploratory and Confirmatory.” The American Statistician 34 (1): 23–25. https://doi.org/10.1080/00031305.1980.10482706.",
    "crumbs": [
      "Research Processes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Hypothesis Generation</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing.html",
    "href": "hypothesis_testing.html",
    "title": "2  Hypothesis Testing",
    "section": "",
    "text": "Test, infer, and (dis)confirm.\nIf you already have specific, hypothesis-ready questions you want to ask about your phenomenon of interest, you are in the domain of confirmatory analysis. In a confirmatory framework, research questions are typically oriented around statistical inferences: conclusions we can draw with some confidence about a general principle or a broader population based on an analysis of sample data.\nTo position ourselves so that we can draw such conclusions with as much confidence as possible, we make use of a wide variety of statistical models and tests, which can be situated within one of two main inferential frameworks: Frequentist inference and Bayesian inference. A deep dive into the comparison between frequentist and Bayesian approaches is beyond the scope of this tutorial—here, we will focus on the frequentist inferential framework, due to its ubiquity and its relevance to common preregistration hurdles (namely, power analysis for sample size estimation).\nNo matter which framework we use, whenever we perform inferential testing in statistical analysis, we are always beholden to certain constraints and assumptions. Statistics is not magic—it is not possible to conjure up the true, cosmic probability of an event occurring or a hypothesis being correct. In some ways, the label “confirmatory” in confirmatory data analysis is a bit over-enthusiastic: statistical inference does not involve true “confirmation”. Instead, it relies on comparisons between observed data and different theoretical scenarios, yielding conditional assessments of the relative likelihoods of those scenarios.\n\n\nInferential testing structure and uncertainty evaluation\n\n\n\nThe most common (and perhaps most notorious, see Cumming 2014; Sakaluk 2016) structure for comparing hypotheses for statistical inference is Null-Hypothesis Significance Testing (NHST). Under NHST, we frame our hypothesis of interest as an “effect”—some relationship between \\(X\\) and \\(Y\\)—and we use our observed data to compare the plausibility of a world in which that effect does not exist (the “null hypothesis” \\(H_{0}\\)) against an alternative world in which that effect exists (the “alternative hypothesis” \\(H_{A}\\)).\nIn other words, inferential questions under NHST will ask, “is the relationship between \\(X\\) and \\(Y\\) equal to zero?” And while these questions are framed to receive a “yes” or “no” answer, it is crucial to remember that our ability to answer them at all relies on the statistical process of estimation, and that process (and the estimates it generates) is always accompanied by some uncertainty.\nIn order for us to provide an answer to our inferential question, we therefore have to specify how much uncertainty we are willing to tolerate. This is where concepts such as Type I and Type II errors (“false positive” and “false negative” conclusions, respectively) come into play. How confident do we have to be that our result is not due to random chance before we are satisfied? In frequentist inference, we call this uncertainty tolerance threshold alpha (\\(\\alpha\\)), and it has to be set in advance. We typically set a threshold of \\(\\alpha = 0.05\\), indicating that we will not tolerate more than a 5% theoretical probability that the null hypothesis is actually true and our result occurred due to random chance.\nWith our uncertainty threshold in hand, we continue with the estimation portion of our statistical modeling process. There are many tools we can use to estimate and quantify relationships in our data—the many flavors of linear regression are among the most common approaches in experimental data analysis, and we will focus on regression models in this tutorial. Importantly, the results of our estimation procedure are not themselves inferential results! The “inference” component of inferential statistical modeling only comes at the point where we assess (or “test”) the uncertainty in our estimates by comparing them to our tolerance threshold. Up until that point, we can estimate and quantify the relationships between \\(X\\) and \\(Y\\) in a purely descriptive manner.\nOnce we are ready to formally compare our estimates of the relationship between \\(X\\) and \\(Y\\) to our hypotheses about that relationship, we enter the domain of inference. It is here, finally, that can we compute a \\(p\\)-value for our estimated effect. With our \\(p\\)-value we are asking, “if we assume the null hypothesis is true and there is no real effect, if this experiment were repeated many many times, what is the probability that we would observe the effect we observed here (or one more extreme)?” If we find that this quantity—which is notoriously difficult to describe succinctly—is below our pre-defined \\(\\alpha\\) threshold value, then we label the result as “statistically significant” and we draw the inference that the relationship between \\(X\\) and \\(Y\\) is not equal to zero.\n\n\nDesigning inferential questions\nHow do you decide which relationship to test for statistical significance? How do you know what to include in your model (what to “control for”)? Generating your precise inferential question depends on your theory of the phenomenon. “Theory” here can be defined as any insight or reasoning that you, as the researcher, provide that guides the construction of your hypothesis. This insight can be derived from your own previous work (including exploratory work), or from existing literature on the phenomenon or related research topics.\nFor example, suppose you have driven a car in different parts of the United States, and you have observed that drivers in northern cities honk their horns more readily in traffic than drivers in southern cities. You are interested in testing this theory formally, because at this point it remains informed only by your personal experience (an exploratory insight). You obtain a dataset of car honks recorded and traffic levels at various intersections in northern and southern cities, and you prepare a research question: is there a difference in honking frequency by U.S. region (north vs. south)?\nThis question might already appear ready for inferential analysis—indeed, you can neatly formulate null and alternative hypotheses for it:\n\\[H_{0}: \\text{Honks}_{\\text{North}} - \\text{Honks}_{\\text{South}} = 0\\] \\[H_{1}: \\text{Honks}_{\\text{North}} - \\text{Honks}_{\\text{South}} ≠ 0 \\] But your theory of the phenomenon goes further: you don’t think drivers honk for no reason, but rather that they are responding to traffic frustration. It is possible, therefore, that a difference in traffic, not the social norms of honking, explains your experiences in the northern and southern U.S. If this were the case, you would want to know! You are not really interested in different regional traffic patterns; you are interested in a possible cultural difference between these regions. Consequently, you refine your research question: is there a difference in honking frequency by U.S. region under equivalent traffic conditions?\nThis refined question requires a more complex model specification, because you have to estimate several quantities at once now. Specifically, you are estimating differences in honk frequency based on two factors simultaneously: region and traffic. This estimation process fits nicely into a multiple regression framework, modeled by the following equation:\n\\[\\text{Honk Frequency} = \\beta_0 + \\beta_1\\text{Region} + \\beta_2\\text{Traffic} + \\epsilon\\] The accompanying null and alternative hypotheses of interest for your inferential test can now be written as:\n\\[H_0: \\beta_1 = 0\\] \\[H_1: \\beta_1 ≠ 0\\] Note that your hypotheses for inferential testing still revolve around the region-related difference in honking frequency (\\(\\beta_1\\)), but the estimate of that difference now accounts for any traffic-related difference in honking frequency (\\(\\beta_2\\)) as another relevant factor.\nWhatever your phenomenon of interest is, and however you derive your theory of it, you, the researcher, will have to make decisions that you must be prepared to justify. As Tukey and other champions of exploratory analysis remind us, the most interesting, most relevant questions do not emerge ready-to-ask from the universe—we have to work to refine them, to coax them into a formal state so that our hypothesis tests generate answers that we are actually interested in learning.\n\n\n\n\nCumming, Geoff. 2014. “The New Statistics: Why and How.” Psychological Science 25 (1): 7–29. https://doi.org/10.1177/0956797613504966.\n\n\nSakaluk, John Kitchener. 2016. “Exploring Small, Confirming Big: An Alternative System to the New Statistics for Advancing Cumulative and Replicable Psychological Research.” Journal of Experimental Social Psychology, Rigorous and replicable methods in social psychology, 66 (September): 47–54. https://doi.org/10.1016/j.jesp.2015.09.013.",
    "crumbs": [
      "Research Processes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "prediction.html",
    "href": "prediction.html",
    "title": "3  Out-of-sample Prediction",
    "section": "",
    "text": "Training and testing\nFinally, there is another approach that is distinct from the testing and inference paradigm which is often used for confirmatory analysis: out-of-sample prediction. This is often the objective in machine learning contexts, where we typically have distinct “training” and “testing” data, and the ultimate aim is to maximize model performance (e.g., classification accuracy) on the testing set.\nOut-of-sample prediction is a confirmatory approach in the sense that it tests a theory, expressed as a model, with new data to see whether the theory holds. In order to implement predictive analyses, however, an exploratory stage is explicitly required in order to derive the model that will be tested for predictive accuracy. Model training, using the designated training data, serves as this exploratory stage of the machine learning workflow. It is also possible to do out-of-sample prediction within a statistical modeling framework—in this case, the exploratory process involves choosing a model specification while modeling an exploratory dataset, and then testing the final model’s performance by comparing its predicted values to real values in a new dataset. Figure 3.1 shows a diagram of this workflow, taken from a predictive analysis of academic performance data by Alboaneen et al. (2022).\n\n\n\n\n\n\nFigure 3.1: Schematic diagram of out-of-sample prediction from Alboaneen et al. (2022)\n\n\n\nThis binary split into a single training dataset and a single testing dataset is the simplest form of out-of-sample prediction analysis, but of course more complex procedures exist as well. In machine learning and classification contexts, for example, various methods are often used for tuning model parameters through intermediate evaluation processes such as cross-validation—the process of splitting up a dataset into many subdivisions and progressively fitting models to a subset of those subdivisions, iteratively testing each model on the held-out subdivision. Procedures like this are used to protect against issues like overfitting to the training dataset, which can reduce the generalizability necessary to achieve good out-of-sample prediction accuracy on testing data. An excellent discussion of this topic is provided by López, López, and Crossa (2022) in Chapter 4 of their online textbook, “Multivariate Statistical Machine Learning Methods for Genomic Prediction”.\n\n\nPrediction vs. expectation\nIn both machine learning and statistical modeling versions of predictive analysis, the confirmatory evaluation of a prediction is an empirical assessment. This means that the assessment of model performance is based on some directly observable quantity—typically the accuracy of the model’s predictions about the testing data. Predictive accuracy is often reported as the proportion of correctly-predicted items or correctly-applied category labels, or else using goodness-of-fit measures to compare model-generated data to the testing data.\nThis empirical assessment approach represents a key difference from confirmatory evaluations under statistical estimation. When we use estimation alone to model observed data directly—such as in a typical regression analysis—we rely on the model to generate plausible “expected values” for the properties of a phenomenon that we cannot directly observe. In other words, we trust the model to tell us what to expect from the broader phenomenon, beyond the data we have. But in the absence of a testing dataset, we cannot operationalize those expectations as predictions per se.\nNevertheless, the language of “prediction” is often used to describe both out-of-sample predictive analyses and estimation-only analyses, which creates a common source of confusion. When a relationship between \\(X\\) and \\(Y\\) is estimated in a regression model, for example, researchers will sometimes describe the resulting estimate—a slope parameter \\(\\beta\\)—by saying that “a one-unit change in \\(X\\) predicts a change of amount \\(\\beta\\) in \\(Y\\).” Indeed, the \\(X\\) variable in such a regression model is often referred to as a “predictor” of \\(Y\\) (which in turn is called the “response” variable). However, when we talk about prediction as a statistical analytical framework, regression and other estimation methods are often not considered to be performing “true” prediction unless their fitted models are applied to a new, independent dataset, and evaluated empirically against that new dataset.\n\n\n\n\nAlboaneen, Dabiah, Modhe Almelihi, Rawan Alsubaie, Raneem Alghamdi, Lama Alshehri, and Renad Alharthi. 2022. “Development of a Web-Based Prediction System for Students’ Academic Performance.” Data 7 (2): 21. https://doi.org/10.3390/data7020021.\n\n\nLópez, Osval Antonio Montesinos, Abelardo Montesinos López, and Dr Jose Crossa. 2022. “Overfitting, Model Tuning, and Evaluation of Prediction Performance.” In. Springer. https://doi.org/10.1007/978-3-030-89010-0_4.",
    "crumbs": [
      "Research Processes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Out-of-sample Prediction</span>"
    ]
  },
  {
    "objectID": "prereg_challenges.html",
    "href": "prereg_challenges.html",
    "title": "Preregistration Challenges",
    "section": "",
    "text": "Now that we’ve reviewed the research workflow with an eye to exploratory and confirmatory frameworks, let’s look at some practical challenges to operating within this framework as a researcher. In the following sections of this tutorial, we’ll discuss two of the most common challenges researchers face when working on a preregistration for a new experiment: deciding on a model specification in advance, and choosing an appropriate sample size for their study.\nBefore we proceed with this discussion, however, it is important to issue the following disclaimer:\nThere are no easy, singular answers to many of the questions that arise around model specification and sample size estimation.\nThese tasks are challenging because they require you, the researcher, to make a lot of decisions and assumptions that you may be reluctant to make. These include decisions like, what covariates do you need to control for? What is the smallest effect size you are interested in? I must apologize in advance if you have come to this tutorial in search of ready-made answers to these questions. Unfortunately, no statistician or consultant can answer these questions for you, because they depend on your knowledge of the phenomenon you study—its formulation in prior literature, its relationships to other phenomena, and its properties as you understand and measure them.\nConsequently, there are very few broad, generalizable standards or “best practices” that can be applied to every analysis in order to justify the “correct” model specification or the “sufficient” sample size. Those general standards and formulas that do exist require many concessions in the form of assumptions and simplifications that may not be appropriate for your case. Navigating the landscape of “standard but unrealistically simplified” to “tailored but impractically complicated” practices is not easy, but the goal of this tutorial is to make it easier by offer you a kind of roadmap.",
    "crumbs": [
      "Preregistration Challenges"
    ]
  },
  {
    "objectID": "model_spec.html",
    "href": "model_spec.html",
    "title": "4  Model Specification",
    "section": "",
    "text": "Asking a question in model form\nNow that we have reviewed the exploratory and confirmatory research workflows in quite a bit of theoretical detail, it should hopefully be clear that formulating a research question that is “statistically answerable” is part of the exploratory research process. As we learned from Tukey previously, there is a key difference between a research idea and a research question, and to use the tools of confirmatory statistics, we need a question.\nFor this section, we will focus on operationalizing research questions as regression models. Regression models are wonderfully flexible, thanks to the many extensions and modifications of the linear model (linear mixed models, generalized linear models, generalized linear mixed models…), as well as the fact that classical statistical tests and analyses (t-tests, ANOVA, \\(\\chi^2\\) tests) can be expressed as special cases of the linear model.\nHowever, with great flexibility comes great responsibility: you, the researcher, must make choices about which model configuration best expresses your question! This sounds obvious, but it can be quite tricky. Which variables will matter? Which covariates or controls might you need to account for? Will you need interaction terms? How will your data be distributed—will you need a generalized linear model (GLM)? Which aspects of your design might require robust methods to address—will you need a mixed-effects model (LMM) or a robust regression model?\nAnswering these questions is the only way to refine a research idea into a model-ready research question, and it can be hard work. Sometimes, your theory or idea alone is not sufficient to resolve all of these unknowns. In those situations, data-driven methods using exploratory data can be extremely valuable to help you translate your research idea into a model-ready research question. These methods can include stepwise regression, regression with shrinkage (e.g., LASSO), or other variable importance analyses. Crucially, data-driven model selection procedures are inherently exploratory, so they should be conducted on exploratory datasets that are separate from the confirmatory data used for inference.\n\n\nThe post-selection inference problem\nWhatever methods you use, you must specify your final model before you can proceed to the inferential testing phase, in order to ensure the integrity of your inferences. If you fail to separate the model selection process from the inferential testing process, your analysis will be vulnerable to the post-selection inference problem.\nA thorough and technical review of post-selection inference is provided by Kuchibhotla, Kolassa, and Kuffner (2022), but for this tutorial we will settle for a high-level description they provide in their introduction (p. 506):\n\n“Once the data have been explored to find the hypothesis or model, the assumptions of a fixed model and fixed hypothesis are no longer appropriate. Classical inference procedures may no longer have the properties established by classical theory. This can invalidate inferences, nullifying the claimed error rates or interpretations. Test statistics and estimators may exhibit distributions completely different from those classical theory prescribes. Biases in estimation caused by data exploration can arise. Procedures designed to control false discovery rates may no longer achieve the desired error control. Power calculations that do not account for data exploration should be viewed suspiciously. The selection of any aspect of a model or hypothesis using the data introduces sampling variability into the model or hypotheses, rendering random the specification process itself.”\n\nIn other words, once you use a dataset to derive the structure of a model, you should not “double-dip” into that same dataset to analyze your model for inference. This is the reason why model specification is a central component of any preregistration.\n\n\nAvoiding post-selection inference\nIf you are able to collect multiple datasets without running into data collection limitations, that is the easiest strategy to achieve both model selection and model testing without running into post-selection inference problems. If you are working with a single dataset and have realized you need to do exploratory, data-driven model selection, you will most likely need to split your data in order to keep your model selection data separate from your testing data. (More complex statistical modeling strategies do exist in as alternatives to data splitting, and they are discussed at length by Kuchibhotla, Kolassa, and Kuffner (2022), but they are difficult to implement technically and they still involve limitations.) Unfortunately, there are no clear guidelines on how to split your data to retain a larger share for the inference dataset, so your best option is to simply divide your dataset in half.\nLearning that you need to split your data after you have already laboriously collected it can be very frustrating. Splitting your data reduces your inferential sample size, and therefore reduces your inferential statistical power (as we’ll talk about in the next section), which can add new limitations to your analysis that you had not anticipated.\nStill, it is important to remember that the exploratory work of refining your research idea into a model-ready research question is an essential part of the iterative research process, and to value it accordingly! Inference is not the only form of learning produced by scientific research. Research programs are cumulative, and the generative output of exploratory work sets us up to ask better, more informative inferential questions in the next round of inquiry and analysis.\n\n\n\n\nKuchibhotla, Arun K., John E. Kolassa, and Todd A. Kuffner. 2022. “Post-Selection Inference.” Annual Review of Statistics and Its Application 9 (1): 505–27. https://doi.org/10.1146/annurev-statistics-100421-044639.",
    "crumbs": [
      "Preregistration Challenges",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model Specification</span>"
    ]
  },
  {
    "objectID": "sample_size.html",
    "href": "sample_size.html",
    "title": "5  Sample Size Estimation",
    "section": "",
    "text": "How many participants do I need (and why)?\n“What sample size do I need?” is almost certainly the most common question new study planners will ask. And for good reason! You need to establish a planned sample size anytime you apply for a grant, complete a preregistration, and ultimately collect study data.\nAnd if you’ve ever asked this question, you have probably been met with the following response: “Do a power analysis to figure out what sample size you need!”\nSounds easy enough, right?\nWell, if you made your way to this tutorial, it’s probably because you found out that this is not as easy or simple as it sounds. But why is it such a challenge?\nFirst of all, just as with model specification, there are several considerations that can make sample size determination a difficult process to standardize. This is because there are actually many parts of an analysis that can be affected by sample size—statistical power is just one of them, and it is specifically related to hypothesis testing and inference.\nOther factors that can depend on sample size include the accuracy and stability of summary statistics (e.g., correlation coefficients, Schönbrodt and Perugini 2013) or parameter estimates in a statistical model (Maas and Hox 2005). And of course, there are practical constraints to sample sizes that we often have to accommodate regardless of statistical ideals, such as when access to a sample is costly or limited, or when a sample comes from a very small or very rare population (such as a patient population for a rare condition or disease). Unlike statistical power, the above factors are important considerations in both exploratory and confirmatory contexts, since they impact aspects of data analysis beyond just inferential testing.\nIt is therefore important to expand our thinking about sample size beyond the scope of power analysis alone. Power analysis can be thought of as a special case of a larger sample size determination framework:\n\nPower analysis to deal with concerns about balancing false positive and false negative rates during statistical inference\nEstimation procedures to deal with the possibility of unstable or biased estimates due to insufficient data, especially in clustered data (see Maas and Hox 2005)\nSampling approaches to deal with practical limitations to sampling effectively\n\nAmong these components, power analysis will often yield the largest minimum sample size requirement. By contrast, practical limitations to sampling (money, time, access) are often the greatest restrictions on potential sample size.\nNavigating these different pieces of the sample-size-estimation puzzle, especially for complex models, is not an exact science. Provide the best justification that you can for your estimated sample size, and preregister that number in advance so that you can preserve the integrity of your statistical inferences during analysis. Finally, remember that there are other ways of validating and increasing confidence in your results and your emergent theory of the phenomenon, such as by conducting replications and out-of-sample prediction analyses. You cannot and should not answer all of your research questions all the way in a single study or analysis!\n\n\nA brief aside: Bayesian sample size estimation\nAlthough we remain primarily focused on frequentist analyses in this tutorial, it is worth taking a short detour to discuss a common question that arises among those doing Bayesian analyses: how do you determine sample sizes for Bayesian models?\nEven though the concept of statistical power is not compatible with the conditional probabilities estimated under Bayesian inference, the challenges of sample size estimation illustrated do not simply disappear under a Bayesian framework. There are still relationships between sample size and important properties of estimation that impact Bayesian models.\nHowever, implementing sample size considerations, especially for effects of interest, is further complicated in Bayesian modeling due to the heavily distributional nature of Bayesian models and their outputs. Unlike frequentist models, which only produce point-estimates of parameter values, Bayesian models yield posterior distributions for each parameter, creating a continuous range of possible inferential tests one can perform. As a result, there are more possible “effects” one can conceptualize for testing with Bayesian models, and therefore more scenarios one must plan for when anticipating the impact of sample size on estimation and inference.\nConsequently, Bayesian sample size estimation is even more difficult to standardize, which partly explains the difficulty of finding resources for how to do it. Generally, the process relies heavily on customized simulation analyses that you will likely have to design for yourself. For at least one illustration of how this can be done in practice, see (Chen and Fraser 2017).\n\n\nPower analysis at last\nNow, finally, we can talk in detail about (frequentist) power analysis. We benefit from power analysis if we are doing confirmatory research where we have:\n\na clear inferential hypothesis,\na fully specified model,\na clearly-defined effect size of interest,\nan inferential testing threshold (e.g., an \\(\\alpha\\) value of 0.05),\na minimum power threshold (e.g., 80% power).\n\nIf you have done power analyses in the past, you are likely familiar with the last three bullet points on that list. Since power analysis involves solving for a missing variable (in our case, sample size), it requires fixing other inferential testing variables in place: the effect size, the acceptable false positive rate (the “Type I error rate”), and the acceptable false negative rate (the “Type II error rate”). However, although the error rates are generally inherited from field standards of practice (generally an \\(\\alpha\\) value of 0.05 or 0.01 and a power value of 80% or 90%), the effect size is derived from our specific hypothesis test—making it dependent on our specific hypothesis and the model we use to test it. And, to complicate matters further, we have to decide, as the researchers, what value for that effect size would be meaningful to us.\nThis is the hardest conceptual part of constructing a power analysis. It’s not difficult to set our threshold for statistical significance in a power analysis—we can simply use our standard \\(\\alpha\\) threshold value. The difficulty is deciding on a threshold for practical significance.\nIn the literature on power analysis, the practical significance threshold value is referred to with the acronym SESOI, which stands for “smallest effect size of interest.” Just as with model specification and inferential question formation, the SESOI is something that you, the researcher, must decide upon. You might be able to source it from existing literature measuring similar effects, or from prior exploratory work of your own. Alternatively, if you have a practical, real-world quantity for an effect that would be meaningful for you to observe, you can construct your SESOI around that quantity.\nFor example, suppose you study performance gaps in educational achievement due to socioeconomic factors, and you are testing the effectiveness of an intervention designed to close those gaps. You might base your SESOI around the effectiveness demonstrated of other interventions in the past, or a percentage of the performance gap that, if closed, would be expected to mitigate important downstream consequences that performance gaps tend to cause.\nOne thing that you should not do when choosing an SESOI is re-use an effect size you’ve observed in a very small pilot sample. For one thing, as discussed above Schönbrodt and Perugini (2013), very small samples can yield unstable or biased estimates of effects, and therefore your SESOI could be anchored on an inaccurate expectation. But more importantly, defining an SESOI is inherently a theoretical task: it is the practical impact you think would matter to observe in your research. You can certainly use exploratory data to give you a realistic range of expectations for effect sizes your phenomenon of interest can reasonably yield, but this is not a substitute for thinking critically about what level of effect would make a difference for our broader understanding of the phenomenon in the world.\n\n\nPower analysis for complex models\nNow armed with your clear inferential hypothesis, your specified model, your SESOI, and your Type I and Type II error rate thresholds, it is finally time to perform a power analysis. At this stage, the conceptual challenges are largely behind you, but procedural challenges remain. The biggest procedural challenge is deciding on the right method for implementing your power analysis. Namely, it is important to use an analysis method that is sufficiently sophisticated for the models you are preparing to use—models which, as discussed in the previous section, are often quite sophisticated themselves!\nClassical power analysis methods were developed for relatively simple inferential testing frameworks, such as t-tests, omnibus F-tests, and simple regression. However, most modern quantitative modeling work extends well beyond these frameworks, frequently involving advanced and complex regression models: the generalized linear model (GLM), the linear mixed-effects model (LMM; also called multilevel or hierarchical linear models), and even the generalized linear mixed-effects model (GLMM).\nStatistical power for advanced regression models—particularly mixed-effects models of any kind—is difficult to compute. The best methods for doing so involve power simulations, since these models are too complex for the analytic solutions available in the case of t-tests and other simpler models.\nThe present tutorial is not itself a power simulation tutorial. But you can find a very thorough tutorial for performing power simulations on (generalized) linear mixed-effects models articulated in this paper (Kumle, Võ, and Draschkow 2021) and their accompanying code notebooks (programmed in R), available here.\n \n\n\n\n \nA particularly useful aspect of the Kumle, Võ, and Draschkow tutorial above is that it includes a specific example for models involving multiple crossed random effects, a common feature of models in psychology and cognitive science (e.g., for studies involving a set of participants each evaluating many stimulus items). In complex models, statistical power becomes sensitive to seemingly subtle changes in design features, including random effect variances and subgroup compositions, which the authors here address in their tutorial paper.\nIf it looks like power simulation is the way forward for you as you plan the power analysis for your research design, you should anticipate that the process will take some time and effort to code up and run. You will have to make a lot of decisions about the distributional structure of your expected data: what are reasonable correlation values for some of the variables? What is the best tool for simulating their values jointly? What proportions will you expect across your categorical data, and how random will they be?\nThere are no easy answers to these questions, but as you spend more time thinking about the properties of datasets, it will become easier to generate and test plausible simulations. Especially if you are new to this process, you will likely benefit from reproducing examples given in tutorials such as the paper linked above, and then adjusting the examples to meet your specifications.\nAnd remember, considerations like time, money, and other costs remain real and valid constraints on your sample size—at the end of the day, you may have to simplify your model and concede to certain assumptions and limitations in order to reach a reasonable, practical sample size estimate for your situation. That does not mean that you won’t be able to do meaningful research! It just means you will have to adjust your expectations and generalizations accordingly, and that your research program may need to grow more before you can address all of your questions of interest.\n\n\n\n\nChen, Ding-Geng, and Mark W. Fraser. 2017. “A Bayesian Approach to Sample Size Estimation and the Decision to Continue Program Development in Intervention Research.” Journal of the Society for Social Work and Research 8 (3): 457–70. https://doi.org/10.1086/693433.\n\n\nKumle, Levi, Melissa L.-H. Võ, and Dejan Draschkow. 2021. “Estimating Power in (Generalized) Linear Mixed Models: An Open Introduction and Tutorial in R.” Behavior Research Methods 53 (6): 2528–43. https://doi.org/10.3758/s13428-021-01546-0.\n\n\nMaas, Cora J. M., and Joop J. Hox. 2005. “Sufficient Sample Sizes for Multilevel Modeling.” Methodology: European Journal of Research Methods for the Behavioral and Social Sciences 1 (3): 86–92. https://doi.org/10.1027/1614-2241.1.3.86.\n\n\nSchönbrodt, Felix D., and Marco Perugini. 2013. “At What Sample Size Do Correlations Stabilize?” Journal of Research in Personality 47 (5): 609–12. https://doi.org/10.1016/j.jrp.2013.05.009.",
    "crumbs": [
      "Preregistration Challenges",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sample Size Estimation</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Alboaneen, Dabiah, Modhe Almelihi, Rawan Alsubaie, Raneem Alghamdi, Lama\nAlshehri, and Renad Alharthi. 2022. “Development of a Web-Based\nPrediction System for Students’ Academic\nPerformance.” Data 7 (2): 21. https://doi.org/10.3390/data7020021.\n\n\nChen, Ding-Geng, and Mark W. Fraser. 2017. “A Bayesian Approach to\nSample Size Estimation and the Decision to Continue Program Development\nin Intervention Research.” Journal of the Society for Social\nWork and Research 8 (3): 457–70. https://doi.org/10.1086/693433.\n\n\nCumming, Geoff. 2014. “The New Statistics: Why and How.”\nPsychological Science 25 (1): 7–29. https://doi.org/10.1177/0956797613504966.\n\n\nFife, Dustin A., and Joseph Lee Rodgers. 2022a. “Understanding the\nExploratory/Confirmatory Data Analysis Continuum: Moving Beyond the\n“Replication Crisis”.” American\nPsychologist 77 (3): 453–66. https://doi.org/10.1037/amp0000886.\n\n\n———. 2022b. “Understanding the Exploratory/Confirmatory Data\nAnalysis Continuum: Moving Beyond the “Replication\nCrisis”.” American Psychologist 77 (3):\n453–66. https://doi.org/10.1037/amp0000886.\n\n\nJacobucci, Ross. 2022. “A Critique of Using the Labels\nConfirmatory and Exploratory in Modern Psychological Research.”\nFrontiers in Psychology 13. https://www.frontiersin.org/articles/10.3389/fpsyg.2022.1020770.\n\n\nJohn, Leslie K., George Loewenstein, and Drazen Prelec. 2012.\n“Measuring the Prevalence of Questionable Research Practices With\nIncentives for Truth Telling.” Psychological Science 23\n(5): 524–32. https://doi.org/10.1177/0956797611430953.\n\n\nKerr, Norbert L. 1998. “HARKing: Hypothesizing After the Results\nAre Known.” Personality and Social Psychology Review 2\n(3): 196–217. https://doi.org/10.1207/s15327957pspr0203_4.\n\n\nKuchibhotla, Arun K., John E. Kolassa, and Todd A. Kuffner. 2022.\n“Post-Selection Inference.” Annual Review of Statistics\nand Its Application 9 (1): 505–27. https://doi.org/10.1146/annurev-statistics-100421-044639.\n\n\nKumle, Levi, Melissa L.-H. Võ, and Dejan Draschkow. 2021.\n“Estimating Power in (Generalized) Linear Mixed Models: An Open\nIntroduction and Tutorial in R.” Behavior Research\nMethods 53 (6): 2528–43. https://doi.org/10.3758/s13428-021-01546-0.\n\n\nLópez, Osval Antonio Montesinos, Abelardo Montesinos López, and Dr Jose\nCrossa. 2022. “Overfitting, Model Tuning, and Evaluation of\nPrediction Performance.” In. Springer. https://doi.org/10.1007/978-3-030-89010-0_4.\n\n\nMaas, Cora J. M., and Joop J. Hox. 2005. “Sufficient Sample Sizes\nfor Multilevel Modeling.” Methodology: European Journal of\nResearch Methods for the Behavioral and Social Sciences 1 (3):\n86–92. https://doi.org/10.1027/1614-2241.1.3.86.\n\n\nMair, Patrick, Joshua S. Cetron, and Ingwer Borg. 2022. “Using\nSupport Vector Machines for Facet Partitioning in Multidimensional\nScaling.” Multivariate Behavioral Research 0 (0): 1–17.\nhttps://doi.org/10.1080/00273171.2022.2035207.\n\n\nNosek, Brian A., Charles R. Ebersole, Alexander C. DeHaven, and David T.\nMellor. 2018. “The Preregistration Revolution.”\nProceedings of the National Academy of Sciences of the United States\nof America 115 (11): 2600–2606. https://doi.org/10.1073/pnas.1708274114.\n\n\nPiper, Mary, Radhika Khetani, Meeta Mistry, Jihe Liu, Amélie Julé,\nGammerdinger, Darío Hereñú, Ilya Sytchev, and kew24. 2023.\nHbctraining/scRNA-Seq_online: Single Cell RNA-Seq\nAnalysis. Zenodo. https://doi.org/10.5281/zenodo.7723318.\n\n\nPollen, Alex A., Tomasz J. Nowakowski, Joe Shuga, Xiaohui Wang, Anne A.\nLeyrat, Jan H. Lui, Nianzhen Li, et al. 2014. “Low-Coverage\nSingle-Cell mRNA Sequencing Reveals Cellular Heterogeneity and Activated\nSignaling Pathways in Developing Cerebral Cortex.” Nature\nBiotechnology 32 (10): 1053–58. https://doi.org/10.1038/nbt.2967.\n\n\nSakaluk, John Kitchener. 2016. “Exploring Small, Confirming Big:\nAn Alternative System to the New Statistics for Advancing Cumulative and\nReplicable Psychological Research.” Journal of Experimental\nSocial Psychology, Rigorous and replicable methods in social\npsychology, 66 (September): 47–54. https://doi.org/10.1016/j.jesp.2015.09.013.\n\n\nSchönbrodt, Felix D., and Marco Perugini. 2013. “At What Sample\nSize Do Correlations Stabilize?” Journal of Research in\nPersonality 47 (5): 609–12. https://doi.org/10.1016/j.jrp.2013.05.009.\n\n\nSimmons, Joseph P., Leif D. Nelson, and Uri Simonsohn. 2011.\n“False-Positive Psychology: Undisclosed Flexibility in Data\nCollection and Analysis Allows Presenting Anything as\nSignificant.” Psychological Science 22 (11): 1359–66. https://doi.org/10.1177/0956797611417632.\n\n\nTukey, John W. 1970. Exploratory Data Analysis. Addison Wesley\nPublishing Company.\n\n\n———. 1980a. “We Need Both Exploratory and Confirmatory.”\nThe American Statistician 34 (1): 23–25. https://doi.org/10.1080/00031305.1980.10482706.\n\n\n———. 1980b. “We Need Both Exploratory and Confirmatory.”\nThe American Statistician 34 (1): 23–25. https://doi.org/10.1080/00031305.1980.10482706.",
    "crumbs": [
      "References"
    ]
  }
]