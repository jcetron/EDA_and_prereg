---
bibliography: references.bib
---

# Hypothesis Generation {.sec-hypothesis_generation}

### Observe, describe, and build hypotheses.

Hypothesis generation is the purview of exploratory work. Hypotheses can sometimes be derived directly from highly specified theories, but in the messy world of the behavioral and social sciences, it is more common to construct hypothesis on the basis of patterns observed in exploratory datasets or selected from our experiences in the world.

<div style="float: right; position: relative; top: 0px; padding: 10px; width: 35%">
[![John Wilder Tukey](images/tukey_3.jpg)](https://csml.princeton.edu/tukey)
</div>

The principle of separating exploratory and confirmatory research into distinct lines of inquiry---crucially, inquiries conducted on *separate* datasets---derives from the original formalization of exploratory data analysis (EDA) developed by **John Tukey** in the 1970s-80s [@tukey1970; @tukey1980]. Tukey championed the value of questions that ask, "what is $X$ like?" or "what characterizes $X$?" These types of questions orient us to analyses that help us document the existence of a phenomenon, describe its features, quantify it, and summarize it.



### Descriptive analysis: charting new territory

In some cases, description and quantification of a phenomenon might itself satisfy your immediate research question. This is often the case when description alone is a considerable challenge, such as when a dataset is especially large or complex. The toolbox of descriptive analysis includes many techniques for summarizing and organizing a dataset to reveal its structure in human-readable terms. 

#### Initial description: summary statistics and distribution visualizations

Let's use an example dataset from the `datasets` package in R to illustrate some of these descriptive analyses in action. The `attitude` dataset contains aggregated data from the Chatterjee-Price attitude survey, in which clerical employees across 30 departments of a large organization were asked to evaluate their employers on a series of topics. Each row of the data frame corresponds to one department, and the values in each column are the percent of favorable responses to each question from across employees sampled in a given department. The dataset is previewed in @tbl-attitude_example_setup (I've renamed the variables based on the [documentation](https://rdrr.io/r/datasets/attitude.html) for interpretability).

```{r tbl-attitude_example_setup, echo=FALSE, message=FALSE}
require(pacman)
p_load(tidyverse, ggplot2, vtable, kableExtra, DT)

# load attitude data
data("attitude")

# rename using full variable names from help file
attitude_varnames_full <- c('Overall rating',
                            'Handling of employee complaints',
                            'Does not allow special privileges',
                            'Opportunity to learn',
                            'Raises based on performance',
                            'Too critical',
                            'Advancement')
colnames(attitude) <- attitude_varnames_full

# data preview
#kable(head(attitude), format = 'html')
datatable(attitude, width = 1200, caption = "Direct preview of the Chatterjee-Price attitude survey data.")
```

As a first step in organizing these data to understand them better, we typically compute *summary statistics*: metrics that help characterize the distributions of our variables. These are generally metrics indicating the location (mean, median, etc.) and dispersion (standard deviation, quartile range, etc.) of a given variable's distribution. @tbl-attitude_example_table is an example summary table for our employee attitude data (computed using the `sumtable()` function from the `vtable` package in R):

```{r tbl-attitude_example_table, echo=FALSE, message=FALSE}
sumtable(attitude, out = 'return') %>% datatable(width = 1000, caption = "Summary statistics table for attitude dataset by variable.")
```


These summary metrics are already an improvement over the raw data alone in our attempt to characterize the data, but we can do much better by adding visualizations. Even simple descriptive visualizations like the histograms shown in @fig-attitude_example_hist can help give us a better impression of each of the distributions in the data.

```{r fig-attitude_example_hist, echo=FALSE, message=FALSE, fig.id=TRUE, fig.cap="Histograms of variable distributions in attitude survey dataset."}
# plot of all histograms
ggplot(data = attitude %>%
         pivot_longer(cols = everything()) %>%
         mutate(name = factor(name, levels = attitude_varnames_full))) +
  geom_histogram(aes(x = value)) +
  xlab('Proportion of favorable responses per department (%)') +
  ggtitle('Histograms of each variable') + 
  facet_wrap(~name, ncol = 2)
```

#### Complex description: multivariate exploratory analyses

Now that we have a better understanding of the individual distributions of each variable, we can use more complex descriptive tools to look at the relationships among the variables, describing the phenomenon more fully. For example, we might compute the relative similarities between each variable and visualize those relationships using tools like cluster analysis, often in conjunction with dimensionality-reduction techniques.

Below is an example of these more advanced exploratory methods applied our attitude data. For this example, we have invented three categories for the variables in the data: "General" for the items "Overall rating" and "Too critical", "Management" for the items "Handling of employee complaints" and "Does not allow special privileges", and "Growth" for the items "Opportunity to learn", "Raises based on performance", and "Advancement". With this category structure in our data, we can combine two techniques to see whether the multivariate relationships in the data reflect the expected category structure. The first technique is multidimensional scaling (MDS), which allows us to embed the relationships between all the variables in a 2-dimensional scatter plot for easy visualization. The second technique is a type of machine learning classification method called support vector machine (SVM) classification, which will try to place the variables into categories based on the relationships in the data. [see @mair2022 for a tutorial paper on the combined use of these two methods.]


```{r fig-attitude_example_svmmds, echo=FALSE, message=FALSE, fig.id=TRUE, fig.cap="Exploratory classification analysis plot showing multidimensional scaling (MDS) used together with support vector machine (SVM) classification on our modified attitude example data."}
p_load(smacof)

# create distance matrix for variables
D <- dist(t(attitude))

# fit MDS
mdsfit <- mds(D, ndim = 2, type = 'ordinal')

# define categories
attitude_categories <- factor(c('General', 'Management', 'Management', 'Growth', 'Growth', 'General', 'Growth'))

# add category information to mds configuration for SVM
attitude_mds_cat <- mdsfit$conf %>%
  as.data.frame() %>%
  mutate(Category = attitude_categories)

# fit SVM (simple example, no tuning)
svmfit <- svm(Category ~ D1 + D2, data = attitude_mds_cat, kernel = 'linear')

# plot joint svm/mds configuration plot
svm_mdsplot(mds_object = mdsfit, svm_object = svmfit, class = attitude_categories, inset = c(-.5, .5))
```

@fig-attitude_example_svmmds shows the results of the exploratory MDS/SVM analysis. The color-coded regions reflect the SVM classifier's predicted category assignments, and the point shapes represent the actual category assignments. As we can see, the data align somewhat well with the expected categories, although not perfectly---the classifier places "Overall rating" and "Opportunity to learn" incorrectly into the "Management" category, due to apparent similarities between those variables and the other "Management" variables.

Multivariate exploratory methods like these can also be used to reveal structures in the data that are not necessarily known in advance, and often in much larger datasets. In genetics research, for example, researchers will characterize cell samples based on the cells' different transcription profiles across thousands or tens of thousands of different genes, hoping to identify sensible structures through exploratory analyses. To achieve a working description of their complex sample data, biologists will use techniques like principal components analysis (PCA) to organize cell samples based on variance in their gene expression patterns, aiming to group together cells that express similar gene profiles [see @marypiper20237723318 for a basic tutorial].

![PCA in genetics research](images/pollen2014_pca_fig.png){#fig-gene_pca}

@fig-gene_pca shows an example of this analysis in action in a genetics paper from @pollen2014 (this figure is discussed as an example in [this](https://www.youtube.com/watch?v=_UVHneBUBW0) tutorial video). In this example, PCA is used to differentiate cell types based on gene transcription heterogeneity.

Methods like PCA, MDS, SVM classification, and other similar techniques are powerful descriptive methods for the current era of "big data", which continues to grow and encompass more and more scientific fields. These methods can help make complex, feature-rich data more approachable and provide a launchpad for the generation of hypotheses.

### From description to hypothesis

In many cases, exploratory description is a prerequisite step to start building up a research program so that you can design subsequent confirmatory analyses. Particularly if your research constitutes the very first inquiry into a new topic area, you might need to dedicate multiple early studies to exploration and description alone before you are ready to generate and test specific hypotheses. Before you can start worrying about statistical power, sample sizes, and *p*-values, you need to decide what question you want to ask, and how it should be formalized into a statistical model or test.

This process---of using exploration to inform hypothesis generation---is complex. And yet, we often mislead ourselves into thinking it is straightforward. To correct this misconception, we can again turn to Tukey, who articulated his concern about this issue in his piece, "We Need Both Exploratory and Confirmatory," published in *The American Statistician* in 1980. In that article, Tukey emphasized that the scientific method is not linear, but *iterative* [@tukey1980a], and illustrated this distinction using a series of diagrams (reproduced below with adjusted formatting):

> "We are, I assert, all too familiar with the following straight-line paradigm---asserted far too frequently as how science and engineering function:
>
> $(*) \; \text{question} \rightarrow \text{design} \rightarrow \text{collection} \rightarrow \text{analysis} \rightarrow \text{answer}$
>
> ...
>
> What often happens is better diagrammed thus:
>
> $(*) \; \text{idea} \rightarrow \, \circlearrowright ({\text{question} \atop \text{design}})\circlearrowright \, \rightarrow \text{collection} \rightarrow \text{analysis} \rightarrow \text{answer}$"

Tukey goes on to explain that there is an important difference between having an *idea* of a question (which we can describe in words) and having a statistically-supportable question (which we can describe in hypothesis terms). In order to obtain a research question that is ready for confirmatory analysis, we may have to repeat the exploratory process many times, refining our question and design along the way.

### Post-hoc exploratory analysis

Finally, exploratory analysis can be a *follow-up* to a completed confirmatory analysis to motivate new, subsequent lines of inquiry [see @fife2022]. It is important to always be clear about exactly *which* analyses are exploratory in this context, so as not to imply that inferential conclusions should be drawn from them. Nevertheless, researchers should not shy away from using exploratory analysis as a generative tool for guiding the long-term development of a research program.
