# Out-of-sample Prediction {#sec-prediction}

### Training and testing

Finally, there is another approach that is distinct from the testing and inference paradigm which is often used for confirmatory analysis: out-of-sample prediction. This is often the objective in machine learning contexts, where we typically have distinct "training" and "testing" data, and the ultimate aim is to maximize model performance (e.g., classification accuracy) on the testing set.

Out-of-sample prediction is a confirmatory approach in the sense that it tests a theory, expressed as a model, with new data to see whether the theory holds. In order to implement predictive analyses, however, an exploratory stage is explicitly required in order to derive the model that will be tested for predictive accuracy. Model training, using the designated training data, serves as this exploratory stage of the machine learning workflow. It is also possible to do out-of-sample prediction within a statistical modeling framework---in this case, the exploratory process involves choosing a model specification while modeling an exploratory dataset, and then testing the final model's performance by comparing its predicted values to real values in a new dataset. @fig-oos_pred shows a diagram of this workflow, taken from a predictive analysis of academic performance data by @alboaneen2022.

![Schematic diagram of out-of-sample prediction from @alboaneen2022](images/The-flowchart-of-the-prediction-model.png){#fig-oos_pred width=60%}

This binary split into a single training dataset and a single testing dataset is the simplest form of out-of-sample prediction analysis, but of course more complex procedures exist as well. In machine learning and classification contexts, for example, various methods are often used for *tuning* model parameters through intermediate evaluation processes such as [*cross-validation*](https://en.wikipedia.org/wiki/Cross-validation_(statistics))---the process of splitting up a dataset into many subdivisions and progressively fitting models to a subset of those subdivisions, iteratively testing each model on the held-out subdivision. Procedures like this are used to protect against issues like *overfitting* to the training dataset, which can reduce the generalizability necessary to achieve good out-of-sample prediction accuracy on testing data. An excellent discussion of this topic is provided by @l√≥pez2022 in [Chapter 4](https://www.ncbi.nlm.nih.gov/books/NBK583970/) of their online textbook, "Multivariate Statistical Machine Learning Methods for Genomic Prediction".

### Prediction vs. expectation

In both machine learning and statistical modeling versions of predictive analysis, the confirmatory evaluation of a prediction is an *empirical* assessment. This means that the assessment of model performance is based on some directly observable quantity---typically the accuracy of the model's predictions about the testing data. Predictive accuracy is often reported as the proportion of correctly-predicted items or correctly-applied category labels, or else using goodness-of-fit measures to compare model-generated data to the testing data.

This empirical assessment approach represents a key difference from confirmatory evaluations under statistical estimation. When we use estimation alone to model observed data directly---such as in a typical regression analysis---we rely on the model to generate plausible "expected values" for the properties of a phenomenon that we cannot directly observe. In other words, we trust the model to tell us what to expect from the broader phenomenon, beyond the data we have. But in the absence of a testing dataset, we cannot operationalize those expectations as predictions *per se*.

Nevertheless, the language of "prediction" is often used to describe both out-of-sample predictive analyses and estimation-only analyses, which creates a common source of confusion. When a relationship between $X$ and $Y$ is estimated in a regression model, for example, researchers will sometimes describe the resulting estimate---a slope parameter $\beta$---by saying that "a one-unit change in $X$ *predicts* a change of amount $\beta$ in $Y$." Indeed, the $X$ variable in such a regression model is often referred to as a "predictor" of $Y$ (which in turn is called the "response" variable). However, when we talk about prediction as a statistical analytical framework, regression and other estimation methods are often not considered to be performing "true" prediction unless their fitted models are applied to a new, independent dataset, and evaluated empirically against that new dataset.
